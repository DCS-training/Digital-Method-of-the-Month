# LLMS 

Schedule: 
- 12:00-12:05 Housekeeping. 
- 12:05-12:10 Introductions from the presenters and attendees. 
- 12:10-12:25 An overview of LLMs.
- 12:25-12:50 Discussion using this document of what LLMs can and cannot do. 
- 12:50-13:00 Discuss available resources, way forwards and wrapping up. 

## Introduction... 

LLMs aren’t a research method but rather a computational tool that can support various research tasks. At their core, LLMs are machine learning models trained on vast amounts of text data, enabling them to generate human-like responses, assist in writing, and analyse language patterns.  While they can support tasks like summarisation, text generation, and automated coding, their capabilities come with limitations. Understanding both their potential and constraints is crucial for effectively integrating them into academic and professional research. 

Some of the most common LLMs are those most frequently mentioned in the media, such as: 

- ChatGPT (Generative Pretrained Transformer) 
- Claude (Anthropic) 
- Gemini (Google) 
- LLaMa (Meta)

The large language models (LLMs) mentioned above all utilize advanced versions of the transformer architecture, which is the foundation of the 'T' in all GPT models. This architecture was first introduced in Google’s 2017 paper, "Attention is All You Need." Since then, it has revolutionised the field of natural language processing (NLP) and serves as the basis for nearly every significant advancement in the area. 

As of this writing, that one single paper by Google has a whopping 172,875 citations, showing the volume of work being done in this space! The current LLM landscape is quickly and constantly evolving, with multiple players all racing past each other to release a bigger, better, faster version of their model. Investors are pouring billions of dollars into NLP companies, with OpenAI alone having raised $11B. 

For now though, we’ll be focusing primarily on instruction-following LLMs (or foundation models), a general purpose class of LLMs that do what you instruct them to. These differ from task-specific LLMs which are fine-tuned for just one task like summarization or translation (to learn more about task-specific models, read our article on use cases and real world applications of LLMs). 

## What can it do? 

- Generate coherent, contextually relevant text. 
- Assist in summarising large volumes of text. 
- Aid in creative tasks such as drafting, editing, and brainstorming.  
- Enhance data-driven research through natural language processing (NLP) capabilities. 
    

## What can it not do? 

- Guarantee factual accuracy without careful oversight and validation. 
- Fully replace human critical analysis and interpretative skills. 
- Perform original research independently; they are systems that augment research rather than replace it. 
- Give coherent answers in a language that it has not had a vast amount of data to be trained from. 
    

## The way forward/training resources. 

1. [Attention Is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)] 
2. [Edinburgh Language Model (ELM)](https://elm.edina.ac.uk/elm/elm)] 
3. [Hackers' Guide to Language Models ](https://www.cdcs.ed.ac.uk/events/silent-disco-hackers-guide-language-models]) 
4. [FreeCodeCamp YouTube LLM Tutorial (For those willing to test their Python skills)](https://www.youtube.com/watch?v=UU1WVnMk4E8) 

## Upcoming CDCS training? 

28th April, 5th May and 12th May: Advanced Use of LLMs with Martin Disley 
(this course will focus on getting attendees to understand how small cases of LLMs work and how they can be run locally).
